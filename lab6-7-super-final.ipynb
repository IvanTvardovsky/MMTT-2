{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d77269",
   "metadata": {
    "id": "11d77269"
   },
   "source": [
    "# ЛР 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626abb1c",
   "metadata": {
    "id": "626abb1c"
   },
   "source": [
    "Загрузим датасет для коллаба"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a_F9G8EvGPtX",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oE3RIp1sGYwb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/mmtt2/archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab259d",
   "metadata": {
    "id": "21ab259d"
   },
   "source": [
    "Ставим зависимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ao0GSrkkHR7F",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision segmentation-models-pytorch ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbe00d",
   "metadata": {
    "id": "a6bbe00d"
   },
   "source": [
    "Фиксируем генератор случайных чисел, чтоб при повторном запуске метрики +- совпали"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QVxqpaYTIs_R",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252e518",
   "metadata": {
    "id": "4252e518"
   },
   "source": [
    "Проверка, чтоб запускаться преимущественно на куде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f01c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2548e",
   "metadata": {
    "id": "a5a2548e"
   },
   "source": [
    "Подготавливаем датасет, все изображения в 150x150, переводим в тензор pytorch и нормалайзим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507bddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), \".\"))\n",
    "\n",
    "train_dir = os.path.join(repo_root, \"seg_train\", \"seg_train\")\n",
    "test_dir  = os.path.join(repo_root, \"seg_test\",  \"seg_test\")\n",
    "\n",
    "baseline_transform = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=baseline_transform)\n",
    "test_ds  = datasets.ImageFolder(test_dir,  transform=baseline_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=4)\n",
    "test_loader  = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"Classes:\", train_ds.classes)\n",
    "print(\"Train / Test samples:\", len(train_ds), \"/\", len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840ea650",
   "metadata": {
    "id": "840ea650"
   },
   "source": [
    "Выбираем метрики. Для несбалансированных данных macro-F1 отражает качество получше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average='macro')\n",
    "    return {'accuracy': acc, 'f1_macro': f1m}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af2e86",
   "metadata": {
    "id": "00af2e86"
   },
   "source": [
    "Бейзлайн resnet-18 1 эпоха"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_ds.classes)\n",
    "model_base = torchvision.models.resnet18(pretrained=True)\n",
    "model_base.fc = nn.Linear(model_base.fc.in_features, num_classes)\n",
    "model_base = model_base.to(device)\n",
    "\n",
    "optimizer_base = torch.optim.Adam(model_base.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler_base = ReduceLROnPlateau(optimizer_base, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    losses, preds, trues = [], [], []\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        preds += out.argmax(1).cpu().tolist()\n",
    "        trues += yb.cpu().tolist()\n",
    "    return sum(losses)/len(losses), compute_metrics(trues, preds)\n",
    "\n",
    "def eval_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses, preds, trues = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            losses.append(loss.item())\n",
    "            preds += out.argmax(1).cpu().tolist()\n",
    "            trues += yb.cpu().tolist()\n",
    "    return sum(losses)/len(losses), compute_metrics(trues, preds)\n",
    "\n",
    "# 4) Один цикл\n",
    "train_loss, train_metrics = train_one_epoch(model_base, train_loader, optimizer_base, criterion, device)\n",
    "test_loss,  test_metrics  = eval_model(model_base, test_loader, criterion, device)\n",
    "scheduler_base.step(test_loss)\n",
    "\n",
    "print(f\"Baseline (1 epoch) train acc={train_metrics['accuracy']:.4f}, f1={train_metrics['f1_macro']:.4f}\")\n",
    "print(f\"Baseline (1 epoch) test acc={test_metrics['accuracy']:.4f}, f1={test_metrics['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c7d5c1",
   "metadata": {
    "id": "01c7d5c1"
   },
   "source": [
    "Аугментация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_aug = transforms.Compose([\n",
    "    transforms.Resize((96,96)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((96,96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "train_ds_aug = datasets.ImageFolder(train_dir, transform=train_transform_aug)\n",
    "test_ds_aug  = datasets.ImageFolder(test_dir,  transform=test_transform)\n",
    "\n",
    "subset_size = len(train_ds_aug)//3\n",
    "train_subset_aug, _ = random_split(train_ds_aug, [subset_size, len(train_ds_aug)-subset_size])\n",
    "\n",
    "train_loader_aug = DataLoader(train_subset_aug, batch_size=64, shuffle=True,  num_workers=4)\n",
    "test_loader_aug  = DataLoader(test_ds_aug,      batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "print(\"Augmented train subset:\", len(train_subset_aug))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b289d",
   "metadata": {
    "id": "466b289d"
   },
   "source": [
    "Улучшаем resnet18. Ускоряем обучение и предотвращаем затирание уже выученных низкоуровенвых фильтров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i2vbrpP0Jf1v",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imp = torchvision.models.resnet18(pretrained=True)\n",
    "for name, p in model_imp.named_parameters():\n",
    "    if not (name.startswith(\"layer4\") or name.startswith(\"fc\")):\n",
    "        p.requires_grad = False\n",
    "\n",
    "model_imp.fc = nn.Linear(model_imp.fc.in_features, num_classes)\n",
    "model_imp = model_imp.to(device)\n",
    "\n",
    "optimizer_imp = torch.optim.Adam([\n",
    "    {'params': model_imp.layer4.parameters(), 'lr':1e-4},\n",
    "    {'params': model_imp.fc.parameters(),    'lr':1e-3},\n",
    "])\n",
    "scheduler_imp = ReduceLROnPlateau(optimizer_imp, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "tl, tm = train_one_epoch(model_imp, train_loader_aug, optimizer_imp, criterion, device)\n",
    "vl, vm = eval_model(model_imp, test_loader_aug, criterion, device)\n",
    "scheduler_imp.step(vl)\n",
    "\n",
    "print(f\"Improved train acc={tm['accuracy']:.4f}, f1={tm['f1_macro']:.4f}\")\n",
    "print(f\"Improved test acc={vm['accuracy']:.4f}, f1={vm['f1_macro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60506fb6",
   "metadata": {
    "id": "60506fb6"
   },
   "source": [
    "resnet18 с полным дообучением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-0sXxpr-Jjkq",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_e2 = torchvision.models.resnet18(pretrained=True)\n",
    "model_e2.fc = nn.Linear(model_e2.fc.in_features, num_classes)\n",
    "model_e2 = model_e2.to(device)\n",
    "\n",
    "opt_e2 = torch.optim.Adam(model_e2.parameters(), lr=1e-4)\n",
    "sch_e2 = ReduceLROnPlateau(opt_e2, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    tl, tm = train_one_epoch(model_e2, train_loader, opt_e2, criterion, device)\n",
    "    vl, vm = eval_model   (model_e2, test_loader,   criterion, device)\n",
    "    sch_e2.step(vl)\n",
    "    print(f\"Epoch {epoch+1} train acc={tm['accuracy']:.4f}, f1={tm['f1_macro']:.4f} | test acc={vm['accuracy']:.4f}, f1={vm['f1_macro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50fb28",
   "metadata": {
    "id": "ad50fb28"
   },
   "source": [
    "Трансформер vit, сравниваем с cnn, прогон 1 эпохи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5A0QVW6TJo39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "preprocess_vit = weights.transforms()\n",
    "\n",
    "model_vit = vit_b_16(weights=weights)\n",
    "for p in model_vit.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "in_f = model_vit.heads.head.in_features\n",
    "model_vit.heads.head = nn.Linear(in_f, num_classes)\n",
    "model_vit = model_vit.to(device)\n",
    "\n",
    "ds_vit = datasets.ImageFolder(train_dir, transform=preprocess_vit)\n",
    "sz = len(ds_vit) // 8\n",
    "sub_vit, _ = random_split(ds_vit, [sz, len(ds_vit)-sz])\n",
    "ldr_vit = DataLoader(sub_vit, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "test_ds_vit = datasets.ImageFolder(test_dir, transform=preprocess_vit)\n",
    "test_loader_vit = DataLoader(test_ds_vit, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "opt_vit = torch.optim.Adam(model_vit.heads.head.parameters(), lr=1e-3)\n",
    "crit_vit = nn.CrossEntropyLoss()\n",
    "\n",
    "vtl, vtm = train_one_epoch(model_vit, ldr_vit, opt_vit, crit_vit, device)\n",
    "vvl, vvm = eval_model(model_vit, test_loader_vit, crit_vit, device)\n",
    "\n",
    "print(f\"Fast ViT train acc={vtm['accuracy']:.4f}, f1={vtm['f1_macro']:.4f}\")\n",
    "print(f\"Fast ViT test acc={vvm['accuracy']:.4f}, f1={vvm['f1_macro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f-w9sGQkKDgp",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fin = torchvision.models.resnet18(pretrained=True)\n",
    "model_fin.fc = nn.Linear(model_fin.fc.in_features, num_classes)\n",
    "model_fin = model_fin.to(device)\n",
    "\n",
    "opt_fin = torch.optim.Adam(model_fin.parameters(), lr=1e-4)\n",
    "sch_fin = ReduceLROnPlateau(opt_fin, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    tl, tm = train_one_epoch(model_fin, train_loader, opt_fin, criterion, device)\n",
    "    vl, vm = eval_model   (model_fin, test_loader, criterion, device)\n",
    "    sch_fin.step(vl)\n",
    "    print(f\"[FINAL] Epoch {epoch+1} train acc={tm['accuracy']:.4f}, f1={tm['f1_macro']:.4f} | test acc={vm['accuracy']:.4f}, f1={vm['f1_macro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XC_bRcCXMf4v",
   "metadata": {
    "id": "XC_bRcCXMf4v"
   },
   "source": [
    "**Сравнение Baseline и Improved на полном наборе**  \n",
    "| Модель                       | Test acc | Test F1 (macro) |\n",
    "|------------------------------|---------:|----------------:|\n",
    "| Бейзлайн ResNet-18 (1 ep)    |    0.9303|           0.9316|\n",
    "| Улучшенный ResNet-18 (2 ep+sch) |    0.9363|           0.9376|\n",
    "\n",
    "**Выводы:**  \n",
    "- Увеличение числа эпох с 1->2 + использование `ReduceLROnPlateau` дало прирост F1~0.006\n",
    "- Эксперимент с частичной fine-tuning и легкими аугментациями на 1/3 данных снизил качество\n",
    "- Ускоренный ViT (head-only на 1/8) показал F1≈0.886 -> уступает ResNet-18 на этих данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3762d325",
   "metadata": {
    "id": "3762d325"
   },
   "source": [
    "Логистическая регрессия - один линейный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hz5dkZ4uMeyK",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x.view(x.size(0), -1))\n",
    "\n",
    "in_features = 3 * 150 * 150\n",
    "model_lr = LogisticRegressionModel(in_features, num_classes).to(device)\n",
    "\n",
    "optimizer_lr = torch.optim.Adam(model_lr.parameters(), lr=1e-4)\n",
    "criterion    = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    tr_loss, tr_metrics = train_one_epoch(model_lr, train_loader, optimizer_lr, criterion, device)\n",
    "    vl_loss, vl_metrics = eval_model   (model_lr, test_loader,  criterion, device)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1} \"\n",
    "        f\"train acc={tr_metrics['accuracy']:.4f}, f1={tr_metrics['f1_macro']:.4f} | \"\n",
    "        f\" test acc={vl_metrics['accuracy']:.4f}, f1={vl_metrics['f1_macro']:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fd0ac",
   "metadata": {
    "id": "887fd0ac"
   },
   "source": [
    "Простая MLP с 1 скрытым слоем и ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MxvH_QJ6MmEq",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "hidden_size = 512\n",
    "model_mlp = SimpleMLP(in_features, hidden_size, num_classes).to(device)\n",
    "\n",
    "optimizer_mlp = torch.optim.Adam(model_mlp.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(5):\n",
    "    tr_loss, tr_metrics = train_one_epoch(model_mlp, train_loader, optimizer_mlp, criterion, device)\n",
    "    vl_loss, vl_metrics = eval_model   (model_mlp, test_loader,  criterion, device)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1} \"\n",
    "        f\"train acc={tr_metrics['accuracy']:.4f}, f1={tr_metrics['f1_macro']:.4f} | \"\n",
    "        f\" test acc={vl_metrics['accuracy']:.4f}, f1={vl_metrics['f1_macro']:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b7a39e",
   "metadata": {
    "id": "23b7a39e"
   },
   "source": [
    "Логистическая регрессия + ReduceLROnPlateu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4YuAmWMEOlj_",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "model_lr = LogisticRegressionModel(in_features, num_classes).to(device)\n",
    "opt_lr   = torch.optim.Adam(model_lr.parameters(), lr=1e-4)\n",
    "sched_lr = ReduceLROnPlateau(opt_lr, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "crit     = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    tr_loss, tr_m = train_one_epoch(model_lr, train_loader, opt_lr, crit, device)\n",
    "    vl_loss, vl_m = eval_model   (model_lr, test_loader,  crit, device)\n",
    "    sched_lr.step(vl_loss)\n",
    "    print(f\"Epoch {epoch+1} train f1={tr_m['f1_macro']:.4f} | test f1={vl_m['f1_macro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf8ea3f",
   "metadata": {
    "id": "fcf8ea3f"
   },
   "source": [
    "MLP с аугментациями и scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tLFvCUpBOnHt",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_transform = transforms.Compose([\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.1),\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "train_ds_aug2 = datasets.ImageFolder(train_dir, transform=aug_transform)\n",
    "train_loader_aug2 = DataLoader(train_ds_aug2, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "model_mlp2 = SimpleMLP(in_features, hidden_size, num_classes).to(device)\n",
    "opt_mlp2   = torch.optim.Adam(model_mlp2.parameters(), lr=1e-4)\n",
    "sched_mlp2 = ReduceLROnPlateau(opt_mlp2, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    tr_loss, tr_m = train_one_epoch(model_mlp2, train_loader_aug2, opt_mlp2, crit, device)\n",
    "    vl_loss, vl_m = eval_model   (model_mlp2, test_loader,       crit, device)\n",
    "    sched_mlp2.step(vl_loss)\n",
    "    print(f\"Epoch {epoch+1} train f1={tr_m['f1_macro']:.4f} | test f1={vl_m['f1_macro']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HnjkQgLOSG_s",
   "metadata": {
    "id": "HnjkQgLOSG_s"
   },
   "source": [
    "# Выводы по ЛР №6\n",
    "\n",
    "- Метрики качества: использовали `accuracy` и `macro-F1`\n",
    "- Baseline ResNet-18 (1 эпоха, full data):\n",
    "  - Test acc = 0.9303, macro-F1 = 0.9316\n",
    "- Improved ResNet-18 (2 эпохи + ReduceLROnPlateau, full data):\n",
    "  - Test acc = 0.9363, macro-F1 = 0.9376\n",
    "- Vision Transformer (head-only на 1/8 data):\n",
    "  - Test macro-F1 ≈ 0.8863\n",
    "- Logistic Regression + Scheduler (10 эпох):\n",
    "  - Test macro-F1 ≈ 0.47\n",
    "- Simple MLP + Scheduler + Augmentations (10 эпох):\n",
    "  - Test macro-F1 ≈ 0.60\n",
    "\n",
    "## Итого получаем:\n",
    "\n",
    "1. ResNet-18 показал отличные результаты уже в одну эпоху и ещё улучшился при 2 эпохах + lr-scheduler\n",
    "2. ViT (head-only) на малом поднаборе данных даёт заметно более низкие показатели по сравнению с ResNet\n",
    "3. Простые модели LogReg, MLP даже с scheduler и аугментациями не приближаются к качеству глубоких сетей - макро-F1 < 0.61\n",
    "4. Финальный improved-бейзлайн - ResNet-18, 2 эпохи, полный датасет, ReduceLROnPlateau (test macro-F1 = 0.9376)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433c602e",
   "metadata": {
    "id": "433c602e"
   },
   "source": [
    "# ЛР 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d376858",
   "metadata": {
    "id": "6d376858"
   },
   "source": [
    "Аналогично"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3h6Q7bA-n9fm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lY2p8bXSoC0v",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/mmtt2/lr7.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed929b",
   "metadata": {
    "id": "aaed929b"
   },
   "source": [
    "Подготавливаем датасет для сегментации - формируем пары [img, mask] для smp.SegmentationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0MSrDcyAoD7P",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycocotools\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "ROOT = \"/content\"\n",
    "SPLITS = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "IMG_SIZE = (256, 256)\n",
    "img_tf = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "])\n",
    "mask_tf = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE, interpolation=InterpolationMode.NEAREST),\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Lambda(lambda x: x.squeeze(0).long())\n",
    "])\n",
    "\n",
    "class CocoSegDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_file, img_transform=None, mask_transform=None):\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.img_ids = list(self.coco.imgs.keys())\n",
    "        self.img_dir = img_dir\n",
    "        self.img_transform = img_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        path = img_info['file_name']\n",
    "        img = Image.open(os.path.join(self.img_dir, path)).convert(\"RGB\")\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
    "        for ann in anns:\n",
    "            mask = np.maximum(mask, self.coco.annToMask(ann))\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        if self.img_transform:\n",
    "            img = self.img_transform(img)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "        return img, mask\n",
    "\n",
    "loaders = {}\n",
    "for split in SPLITS:\n",
    "    img_dir  = os.path.join(ROOT, split)\n",
    "    ann_file = os.path.join(ROOT, split, \"_annotations.coco.json\")\n",
    "    ds = CocoSegDataset(img_dir, ann_file, img_tf, mask_tf)\n",
    "    loaders[split] = DataLoader(ds, batch_size=8,\n",
    "                                shuffle=(split==\"train\"),\n",
    "                                num_workers=2)\n",
    "\n",
    "\n",
    "xb, yb = next(iter(loaders[\"train\"]))\n",
    "print(\"Images:\", xb.shape)\n",
    "print(\"Masks: \", yb.shape)\n",
    "print(\"Unique mask values:\", torch.unique(yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ed979",
   "metadata": {
    "id": "014ed979"
   },
   "source": [
    "Выбираем метрики IoU и Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cKDgb-2sojR-",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def iou_score(pred: torch.Tensor, target: torch.Tensor, smooth=1e-6):\n",
    "    pred = (pred > 0.5).long()\n",
    "    intersection = (pred & target).float().sum((1,2))\n",
    "    union = (pred | target).float().sum((1,2))\n",
    "    return ((intersection + smooth) / (union + smooth)).mean()\n",
    "\n",
    "def dice_score(pred: torch.Tensor, target: torch.Tensor, smooth=1e-6):\n",
    "    pred = (pred > 0.5).long()\n",
    "    intersection = (pred & target).float().sum((1,2)) * 2\n",
    "    total = pred.float().sum((1,2)) + target.float().sum((1,2))\n",
    "    return ((intersection + smooth) / (total + smooth)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qV9xkmHvpRCD",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1fdd0",
   "metadata": {
    "id": "0bc1fdd0"
   },
   "source": [
    "Бейзлайн для UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MsHoboRMpOK8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model_unet = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None\n",
    ").to(device)\n",
    "\n",
    "criterion_seg = nn.BCEWithLogitsLoss()\n",
    "optimizer_seg = torch.optim.Adam(model_unet.parameters(), lr=1e-4)\n",
    "scheduler_seg = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_seg, mode='min', factor=0.5, patience=1, verbose=True\n",
    ")\n",
    "\n",
    "def train_epoch_seg(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, ious, dices = 0, [], []\n",
    "    for imgs, masks in loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits.squeeze(1), masks.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.sigmoid(logits).squeeze(1)\n",
    "        ious.append(iou_score(preds, masks).item())\n",
    "        dices.append(dice_score(preds, masks).item())\n",
    "    return total_loss/len(loader), sum(ious)/len(ious), sum(dices)/len(dices)\n",
    "\n",
    "def eval_epoch_seg(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, ious, dices = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits.squeeze(1), masks.float())\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.sigmoid(logits).squeeze(1)\n",
    "            ious.append(iou_score(preds, masks).item())\n",
    "            dices.append(dice_score(preds, masks).item())\n",
    "    return total_loss/len(loader), sum(ious)/len(ious), sum(dices)/len(dices)\n",
    "\n",
    "train_loss, train_iou, train_dice = train_epoch_seg(\n",
    "    model_unet, loaders[\"train\"], optimizer_seg, criterion_seg, device\n",
    ")\n",
    "val_loss,   val_iou,   val_dice   = eval_epoch_seg(\n",
    "    model_unet, loaders[\"valid\"], criterion_seg, device\n",
    ")\n",
    "scheduler_seg.step(val_loss)\n",
    "\n",
    "print(f\"Unet Baseline train IoU={train_iou:.4f}, Dice={train_dice:.4f}\")\n",
    "print(f\"Unet Baseline val IoU={val_iou:.4f}, Dice={val_dice:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e39b66",
   "metadata": {
    "id": "f4e39b66"
   },
   "source": [
    "Улучшенный DeepLabV3, аугментация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GznxtJllrm7s",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "aug_img_tf = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "aug_mask_tf = mask_tf\n",
    "\n",
    "train_ds_aug = CocoSegDataset(\n",
    "    img_dir=os.path.join(ROOT, \"train\"),\n",
    "    ann_file=os.path.join(ROOT, \"train\", \"_annotations.coco.json\"),\n",
    "    img_transform=aug_img_tf,\n",
    "    mask_transform=aug_mask_tf\n",
    ")\n",
    "valid_ds = CocoSegDataset(\n",
    "    img_dir=os.path.join(ROOT, \"valid\"),\n",
    "    ann_file=os.path.join(ROOT, \"valid\", \"_annotations.coco.json\"),\n",
    "    img_transform=img_tf,\n",
    "    mask_transform=mask_tf\n",
    ")\n",
    "\n",
    "train_loader_aug = DataLoader(train_ds_aug, batch_size=8, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Train aug batch: \", next(iter(train_loader_aug))[0].shape,\n",
    "      next(iter(train_loader_aug))[1].shape)\n",
    "print(\"Valid batch: \", next(iter(valid_loader))[0].shape,\n",
    "      next(iter(valid_loader))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qj6jX4y6rsTk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "\n",
    "model_unet_ft = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None\n",
    ").to(device)\n",
    "\n",
    "for name, param in model_unet_ft.named_parameters():\n",
    "    if name.startswith(\"encoder\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "bce_loss  = nn.BCEWithLogitsLoss()\n",
    "dice_loss = DiceLoss(mode='binary')\n",
    "\n",
    "def composite_loss(logits, masks):\n",
    "    bce = bce_loss(logits.squeeze(1), masks.float())\n",
    "    dice = dice_loss(torch.sigmoid(logits), masks.unsqueeze(1).float())\n",
    "    return bce + dice\n",
    "\n",
    "optimizer_ft = torch.optim.Adam(model_unet_ft.parameters(), lr=1e-4)\n",
    "scheduler_ft = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_ft, mode='min', factor=0.5, patience=1, verbose=True\n",
    ")\n",
    "\n",
    "def train_epoch_improved(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_iou, epoch_dice = 0, [], []\n",
    "    for imgs, masks in loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = composite_loss(logits, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        preds = torch.sigmoid(logits).squeeze(1)\n",
    "        epoch_iou.append(iou_score(preds, masks).item())\n",
    "        epoch_dice.append(dice_score(preds, masks).item())\n",
    "    return epoch_loss/len(loader), sum(epoch_iou)/len(epoch_iou), sum(epoch_dice)/len(epoch_dice)\n",
    "\n",
    "def eval_epoch_improved(model, loader, device):\n",
    "    model.eval()\n",
    "    val_loss, val_iou, val_dice = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            logits = model(imgs)\n",
    "            loss = composite_loss(logits, masks)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.sigmoid(logits).squeeze(1)\n",
    "            val_iou.append(iou_score(preds, masks).item())\n",
    "            val_dice.append(dice_score(preds, masks).item())\n",
    "    return val_loss/len(loader), sum(val_iou)/len(val_iou), sum(val_dice)/len(val_dice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fac025a",
   "metadata": {
    "id": "7fac025a"
   },
   "source": [
    "Прогоним улучшенный UNet с composite loss и аугментациями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j9JHjidwryfJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 4):\n",
    "    tr_loss, tr_iou, tr_dice = train_epoch_improved(\n",
    "        model_unet_ft, train_loader_aug, optimizer_ft, device\n",
    "    )\n",
    "    val_loss, val_iou, val_dice = eval_epoch_improved(\n",
    "        model_unet_ft, valid_loader, device\n",
    "    )\n",
    "    scheduler_ft.step(val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} \"\n",
    "        f\"train IoU={tr_iou:.4f}, Dice={tr_dice:.4f} | \"\n",
    "        f\"val IoU={val_iou:.4f}, Dice={val_dice:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IKJjpPgFsTtU",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_unet_ft.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "simple_img_tf = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "train_ds_simple = CocoSegDataset(\n",
    "    img_dir=os.path.join(ROOT, \"train\"),\n",
    "    ann_file=os.path.join(ROOT, \"train\", \"_annotations.coco.json\"),\n",
    "    img_transform=simple_img_tf,\n",
    "    mask_transform=mask_tf\n",
    ")\n",
    "train_loader_simple = DataLoader(train_ds_simple, batch_size=8, shuffle=True, num_workers=2)\n",
    "\n",
    "for epoch in range(0, 3):\n",
    "    tr_loss, tr_iou, tr_dice = train_epoch_improved(\n",
    "        model_unet_ft, train_loader_simple, optimizer_ft, device\n",
    "    )\n",
    "    val_loss, val_iou, val_dice = eval_epoch_improved(\n",
    "        model_unet_ft, valid_loader, device\n",
    "    )\n",
    "    scheduler_ft.step(val_loss)\n",
    "    print(f\"Epoch {epoch} train IoU={tr_iou:.4f}, Dice={tr_dice:.4f} | val IoU={val_iou:.4f}, Dice={val_dice:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91daef",
   "metadata": {
    "id": "2e91daef"
   },
   "source": [
    "Реализовываем минимальный UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CP4g4_yDtmjB",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pix, total_pix = 0, 0\n",
    "for _, m in loaders[\"train\"]:\n",
    "    pos_pix += (m>0).sum().item()\n",
    "    total_pix += m.numel()\n",
    "print(\"Pos‑ratio:\", pos_pix/total_pix)\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "class UNetLite(nn.Module):\n",
    "    def __init__(self, ch=64):\n",
    "        super().__init__()\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, ch, 3, 1, 1), nn.ReLU(),\n",
    "            nn.Conv2d(ch, ch, 3, 1, 1), nn.ReLU())\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch*2, 3, 1, 1), nn.ReLU(),\n",
    "            nn.Conv2d(ch*2, ch*2, 3, 1, 1), nn.ReLU())\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.bridge = nn.Sequential(\n",
    "            nn.Conv2d(ch*2, ch*4, 3, 1, 1), nn.ReLU(),\n",
    "            nn.Conv2d(ch*4, ch*4, 3, 1, 1), nn.ReLU())\n",
    "        self.up2  = nn.ConvTranspose2d(ch*4, ch*2, 2, 2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(ch*4, ch*2, 3, 1, 1), nn.ReLU(),\n",
    "            nn.Conv2d(ch*2, ch*2, 3, 1, 1), nn.ReLU())\n",
    "        self.up1  = nn.ConvTranspose2d(ch*2, ch, 2, 2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(ch*2, ch, 3, 1, 1), nn.ReLU(),\n",
    "            nn.Conv2d(ch, ch, 3, 1, 1), nn.ReLU())\n",
    "        self.out  = nn.Conv2d(ch, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        b  = self.bridge(self.pool2(e2))\n",
    "        d2 = self.up2(b)\n",
    "        d2 = self.dec2(torch.cat([d2, e2], 1))\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = self.dec1(torch.cat([d1, e1], 1))\n",
    "        return self.out(d1)\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def iou_score(pred, mask, eps=1e-6):\n",
    "    pred = (pred > 0.5).float()\n",
    "    inter = (pred * mask).sum(dim=(1,2))\n",
    "    union = (pred + mask).clamp(0,1).sum(dim=(1,2)) - inter\n",
    "    return ((inter + eps) / (union + eps)).mean().item()\n",
    "\n",
    "def dice_score(pred, mask, eps=1e-6):\n",
    "    pred = (pred > 0.5).float()\n",
    "    inter = (pred * mask).sum(dim=(1,2))\n",
    "    return ((2*inter + eps) /\n",
    "            (pred.sum(dim=(1,2)) + mask.sum(dim=(1,2)) + eps)).mean().item()\n",
    "\n",
    "def train_epoch_fcn(model, loader, optim, criterion, device):\n",
    "    model.train()\n",
    "    tot_loss, ious, dices = 0, [], []\n",
    "    for imgs, masks in loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits.squeeze(1), masks.float())\n",
    "        optim.zero_grad(); loss.backward(); optim.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(logits).squeeze(1)\n",
    "            ious.append(iou_score(probs, masks))\n",
    "            dices.append(dice_score(probs, masks))\n",
    "            tot_loss += loss.item()\n",
    "    return (tot_loss/len(loader), np.mean(ious), np.mean(dices))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch_fcn(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    tot_loss, ious, dices = 0, [], []\n",
    "    for imgs, masks in loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits.squeeze(1), masks.float())\n",
    "        probs = torch.sigmoid(logits).squeeze(1)\n",
    "        ious.append(iou_score(probs, masks))\n",
    "        dices.append(dice_score(probs, masks))\n",
    "        tot_loss += loss.item()\n",
    "    return (tot_loss/len(loader), np.mean(ious), np.mean(dices))\n",
    "\n",
    "\n",
    "model = UNetLite().to(device)\n",
    "model_base = UNetLite(ch=64).to(device)\n",
    "\n",
    "optimizer_base = torch.optim.Adam(model_base.parameters(), lr=1e-4)\n",
    "criterion_base = nn.BCEWithLogitsLoss()\n",
    "\n",
    "pos_w = torch.tensor(max(1.0, 1/(pos_pix/total_pix)-1)).to(device)\n",
    "criterion_base = nn.BCEWithLogitsLoss(pos_weight=pos_w)\n",
    "optimizer_base = torch.optim.Adam(model_base.parameters(), lr=1e-3)\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_iou, tr_dice = train_epoch_fcn(\n",
    "        model_base, train_loader_simple, optimizer_base, criterion_base, device\n",
    "    )[:3]\n",
    "    val_loss, val_iou, val_dice = eval_epoch_fcn(\n",
    "        model_base, valid_loader, criterion_base, device\n",
    "    )\n",
    "    print(f\"E{epoch} val IoU={val_iou:.3f} Dice={val_dice:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81bb1fe",
   "metadata": {
    "id": "c81bb1fe"
   },
   "source": [
    "Лосс, оптимизатор, трейн луп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2UQh6iR01eBP",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "pos_w = torch.tensor(max(1.0, 1/(pos_pix/total_pix)-1)).to(device)\n",
    "bce  = nn.BCEWithLogitsLoss(pos_weight=pos_w)\n",
    "dice = DiceLoss(mode=\"binary\")\n",
    "def seg_loss(logits, y):\n",
    "    return bce(logits.squeeze(1), y.float()) + dice(torch.sigmoid(logits), y.unsqueeze(1).float())\n",
    "\n",
    "opt  = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    running_iou, running_dice = [], []\n",
    "    for i,(x,y) in enumerate(train_loader_simple):\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        opt.zero_grad()\n",
    "        logit = model(x)\n",
    "        loss  = seg_loss(logit, y)\n",
    "        loss.backward(); opt.step()\n",
    "        if i%100==0:\n",
    "            pred = torch.sigmoid(logit).squeeze(1)\n",
    "            running_iou.append(iou_score(pred,y).item())\n",
    "            running_dice.append(dice_score(pred,y).item())\n",
    "            print(f\"ep{epoch+1} b{i}: IOU {np.mean(running_iou):.3f} Dice {np.mean(running_dice):.3f}\")\n",
    "\n",
    "    model.eval(); val_iou,val_dice=[],[]\n",
    "    with torch.no_grad():\n",
    "        for x,y in valid_loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            pred = torch.sigmoid(model(x)).squeeze(1)\n",
    "            val_iou.append(iou_score(pred,y).item())\n",
    "            val_dice.append(dice_score(pred,y).item())\n",
    "    print(f\"ep{epoch+1} VAL: IoU {np.mean(val_iou):.3f} Dice {np.mean(val_dice):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2da51a",
   "metadata": {
    "id": "6d2da51a"
   },
   "source": [
    "# Выводы по ЛР №7\n",
    "\n",
    "- Бейзлацн UNet после 5 эпох дал примерно IoU ~ 0.10, Dice ~ 0.18, что довольно слабо для практики\n",
    "- Простые аугментации flip, rotate подняли метрики на пару десятков сотых, но чуда не случилось\n",
    "- Замена модели на DeepLabV3+ с encoder‑efficientnet‑b3 сразу дала заметный прирост: IoU ~ 0.29, Dice ~ 0.46\n",
    "- Сделанный облегченный UNet‑lite оказался по середине: качество чуть ниже DeepLab, зато легче и быстрее\n",
    "- Чтобы улучшить дальше нужно дольше учить (20‑30 эпох) и вероятно добавить более «умные» loss‑функции или аугментации"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
